# Feature: Resident Scheduler (Cron) for Mister Morph
Date: 2026-02-03

## Summary
Add a first-class, **long-running (resident) scheduler** to Mister Morph that can execute agent tasks on a time-based schedule (cron and/or fixed interval) with **persistent state**. This feature is designed for a **single-instance deployment** (no distributed locking required).

The scheduler runs inside the Mister Morph process, triggers jobs according to their schedule, records run history, and exposes an internal API (and/or internal tool entrypoints) for the agent to manage scheduled jobs.


## User Requirements (Confirmed)
- **Resident process**: the scheduler runs continuously.
- **Persistence**: jobs and run history must survive restarts.
- **Single instance**: there will not be multiple scheduler instances.
- **UTC time**: schedules are evaluated in UTC.
- **Misfire policy**: hardcoded to `skip` (no catch-up runs after downtime).
- **Retries**: not required.
- **Retention**: no automatic cleanup initially.
- **Default timeout**: hardcoded to 10 minutes (per-job `timeout_seconds` can override).
- **Idle load**: when there are no queued runs, workers should not tight-poll the DB (wake-on-enqueue; idle checks are bounded).

## Goals
- Schedule tasks using cron-like expressions and/or fixed intervals.
- Persist:
  - job definitions (what to run and when),
  - run history (status, timestamps, errors, outputs/metadata),
  - last/next run times (for observability and restart behavior).
- Ensure safe execution semantics (timeouts, retries, non-overlapping runs by default).
- Keep provider-specific behavior in `providers/<name>/`, not in the scheduler core.

## Non-Goals
- Distributed scheduling / leader election / multi-instance coordination.
- Exactly-once semantics across crashes for side-effecting tools (best-effort with idempotency guidance).
- User-facing CLI management commands or a web UI.
- A general workflow engine (this is time-triggered task execution, not DAG orchestration).

## Agent-Facing API (No Public CLI)
This capability is intended to be called internally by the agent (or by internal tools/hooks), not exposed as user-facing CLI commands.

Two viable integration shapes:
1) **In-process scheduler module**: `agent` starts a scheduler goroutine at startup and registers jobs programmatically.
2) **Internal tools**: expose built-in tools callable by the agent; these tools are not necessarily exposed to end users.

Implemented internal tools (when `scheduler.enabled=true`):
- `schedule_job`: create/update a job by exact `name` (upsert)
- `list_jobs`: list recent jobs (no matching) so the agent can pick one
- `search_jobs`: search jobs by substring keywords and optional UTC time filters (to find “the 8am news job from yesterday”)
- `unschedule_job`: disable (default) or delete a job by `job_id` or exact `name`

### Job spec fields
Minimum:
- `name`: human-friendly name
- `schedule`: cron expression (recommended) OR `interval_seconds` (fixed interval in seconds; repeats unless `run_once=true`)
- `task`: the task text passed to the agent (same as `run --task`)

Recommended:
- `enabled`: bool (default true)
- `run_once`: if true, disable the job after its next scheduled enqueue (one-shot execution)
- `notify_telegram_chat_id`: optional Telegram `chat_id` to notify after each run (best-effort; depends on runtime wiring)
- `timeout_seconds`: per-run hard timeout
- `overlap_policy`: `forbid` | `queue` | `replace` (default `forbid`)
- `provider`, `model`: optional overrides (fallback to config defaults)
- `labels`: arbitrary tags for filtering

### Examples
Example: schedule a daily job at 09:00 UTC (pseudocode):
```go
scheduler.UpsertJob(Job{
  Name: "daily-repo-summary",
  Schedule: "0 9 * * *",
  Task: "Summarize changes since yesterday.",
  TimeoutSeconds: 600,
})
```

## Persistence Model

### Storage backend
Use the existing project database configuration (`db.dsn`) and store scheduler state in SQLite tables.

Rationale:
- Single-instance + persistent state is a strong fit for SQLite.
- Easy inspection/debugging with standard tools.
- Supports atomic updates for run status transitions.

Note: this feature does not require writing per-run artifacts to disk; persistence refers to scheduler metadata and run history stored in SQLite.

### Proposed schema (SQLite)
Tables:

`cron_jobs`
- `id` (TEXT, primary key; opaque string id)
- `name` (TEXT)
- `enabled` (INTEGER 0/1)
- `schedule` (TEXT nullable) — cron expression
- `interval_seconds` (INTEGER nullable)
- `task` (TEXT)
- `run_once` (INTEGER 0/1) — if true, disable after the next enqueue
- `notify_telegram_chat_id` (INTEGER nullable) — Telegram chat id to notify after each run
- `provider` (TEXT nullable)
- `model` (TEXT nullable)
- `timeout_seconds` (INTEGER nullable)
- `overlap_policy` (TEXT)
- `created_at` (INTEGER unix seconds)
- `updated_at` (INTEGER unix seconds)

`cron_runs`
- `id` (TEXT, primary key; opaque string id)
- `job_id` (TEXT, indexed; FK to `cron_jobs.id`)
- `status` (TEXT: `queued|running|succeeded|failed|canceled|timed_out|skipped`)
- `scheduled_for` (INTEGER unix seconds)
- `started_at` (INTEGER nullable)
- `finished_at` (INTEGER nullable)
- `attempt` (INTEGER) — 1..N
- `error` (TEXT nullable) — last error message
- `result_summary` (TEXT nullable) — optional small summary (bounded); full outputs are not persisted

Optional:

`cron_kv`
- `key` (TEXT primary key)
- `value` (TEXT)

Used for small global state such as schema version, last scheduler start time, etc.

### ID generation
IDs are stored as `TEXT` and should be treated as opaque identifiers.

Chosen strategy: **UUID** (UUIDv4 or UUIDv7) for both `cron_jobs.id` and `cron_runs.id`.

Note: even with UUID, keep `created_at/updated_at` and order by timestamps for clarity.

## Run History Contents
Run history exists to support:
- restart recovery (“what was running when the process died?”),
- operational visibility (“is this job healthy?”),
- debugging (“why did it fail, when, and how many retries?”).

This feature intentionally does **not** persist full agent transcripts or tool I/O. A run record should be small and bounded.

Recommended fields to persist per run (in addition to IDs):
- **Timing**: `scheduled_for`, `started_at`, `finished_at` (or derived duration)
- **State**: `status`, `attempt`
- **Failure info** (only on failure): a short `error` string (bounded/truncated)
- **Inputs snapshot (minimal)**:
  - either store `task` redundantly in `cron_runs` (simplest),
  - or store `job_updated_at` / `job_version` to know exactly which job definition produced the run.
- **Optional outcome hint**: `result_summary` (short, bounded; e.g. first 200–1000 chars), stored in `cron_runs.result_summary`

### Example run records
Succeeded:
```json
{
  "id": "c6d9f1a6-0f1b-4f9b-a3a8-1d6e7a2e9c01",
  "job_id": "4b3a0f59-b6c2-4c8c-9b2c-0d9d68bd9a10",
  "status": "succeeded",
  "scheduled_for": 1769994000,
  "started_at": 1769994002,
  "finished_at": 1769994017,
  "attempt": 1,
  "error": null,
  "result_summary": "Completed daily summary; 3 files changed; action items: update docs and run go vet."
}
```

Failed:
```json
{
  "id": "a1f1d8c7-7c2d-4a67-9a1a-1dba0fb10b62",
  "job_id": "4b3a0f59-b6c2-4c8c-9b2c-0d9d68bd9a10",
  "status": "failed",
  "scheduled_for": 1769997600,
  "started_at": 1769997603,
  "finished_at": 1769997610,
  "attempt": 1,
  "error": "timeout: agent step exceeded 600s deadline",
  "result_summary": null
}
```

Skipped (overlap forbidden):
```json
{
  "id": "2c3b9e0d-7a38-4a07-9e27-2a2a6a3c2f9b",
  "job_id": "4b3a0f59-b6c2-4c8c-9b2c-0d9d68bd9a10",
  "status": "skipped",
  "scheduled_for": 1769998200,
  "started_at": null,
  "finished_at": null,
  "attempt": 1,
  "error": "overlap_forbid: prior run still running",
  "result_summary": null
}
```

## Scheduling & Execution Semantics

### Triggering
The scheduler maintains an in-memory schedule (cron parser and/or interval ticker) but **sources truth from the DB** (SQLite):
- On start, load all enabled jobs from the DB.
- Watch for job changes via periodic reload (e.g. every 5–30s) or a simple “version bump” (`updated_at`) check.
  - Single instance means no cross-process notification needed.

### Tick and wake semantics
The scheduler has a configurable **poll tick** (`scheduler.tick`, a Go duration string; default `60s`). Smaller values improve schedule precision but increase DB checks.

Workers are **wake-on-enqueue**: when the scheduler inserts a `cron_runs` row in `queued` status, it wakes workers so they can claim and execute immediately. When idle (no queued runs), worker checks are bounded (no tight polling).

### Time standard (UTC)
All schedule evaluation (cron or interval) is performed in **UTC**.

### Misfire behavior (missed runs)
On restart, some schedules may have elapsed.
Hardcoded behavior: `skip`. Do not “catch up”; schedule the next future time only.

Avoid unbounded catch-up loops.

### Overlap policy
Overlap policy defines what happens when a job is triggered but there is already an active run of the same job.

Recommended default: **`forbid`** overlapping runs per job.
- `forbid`: if a run is already `running`, emit a `slog` record (e.g. job_id, running_run_id, scheduled_for) and persist a `cron_runs` row with `status="skipped"`.
- `queue`: if running, enqueue one pending run (or enqueue all, bounded by a max queue depth).
- `replace`: cancel running run (best-effort) and start the new run.

Given misfire behavior is hardcoded to `skip` and there are no retries, `forbid` is the simplest and safest behavior. Add `queue` later only if you require “eventual execution” (never miss a scheduled tick while the job is slow).

### Concurrency model
Use a worker pool with a global concurrency limit (configurable).
- Scheduling produces `cron_runs` rows with `queued` status (or `skipped` when overlap is forbidden).
- Workers claim queued runs by atomically transitioning `queued -> running`.
- Runs transition to terminal states on completion.

### Timeouts, cancellation, retries
- Enforce `timeout_seconds` via context deadlines.
- Retries are not required in the current scope; failed runs remain failed and are not automatically re-enqueued.

## Integration with Existing Agent

### How a scheduled run maps to current behavior
A scheduled run should execute the same code path as the existing “run one task” behavior (the same agent loop, provider selection, and tool execution), just invoked by the scheduler.

Inputs for a run are derived from:
- job’s `task` + optional overrides (`provider`, `model`)
- standard config (`config.yaml`) and env vars (e.g. `MISTER_MORPH_API_KEY`)

### How cron jobs interact with the agent
The scheduler and the agent interact through a small in-process interface. The scheduler itself should not call the LLM; it only decides *when* to start a run and *with what inputs*.

Recommended integration (single process):
- Scheduler creates a `cron_runs` row (`queued`), then a worker claims it and invokes the agent runner.
- The agent runner executes the normal agent loop with a context that includes scheduler metadata.

Minimal data passed from scheduler to the agent runner:
- `run_id`, `job_id`
- `scheduled_for` (UTC)
- `trigger`: `"cron"`
- `task` (and optionally provider/model overrides)

How the agent “knows cron exists” depends on the mode you choose:
1) **Passive (recommended default)**: the agent does not need to know about scheduling at all. It simply receives a task and runs it. Cron is an external trigger.
2) **Context-aware**: inject a small, non-sensitive note into the run context/system prompt (recommended here). This helps the agent choose non-interactive behavior and produce the desired output format.
3) **Agent-managed scheduling (optional, future)**: expose internal tools like `schedule_job` / `disable_job` so the agent can create/update jobs. If enabled, explicitly tell the agent these tools exist and define policy constraints (who is allowed to schedule what, limits, etc.).

#### Metadata injection structure
There is no universally standardized “run metadata” structure in the current agent API. To support cron and other future triggers (Telegram, daemon queue, webhooks, etc.), introduce a single consistent mechanism:

Recommended approach:
- Add `RunOptions.Meta map[string]any` (or similar) as the standard metadata carrier for **all** runs.
- Convert `RunOptions.Meta` into **one** injected `user` message before the task.
- Use a stable envelope to avoid key collisions (see `docs/feat/feat_20260203_meta.md`), e.g.:
```json
{
  "mister_morph_meta": {
    "trigger": "cron",
    "cron_job_id": "4b3a0f59-b6c2-4c8c-9b2c-0d9d68bd9a10",
    "cron_run_id": "c6d9f1a6-0f1b-4f9b-a3a8-1d6e7a2e9c01",
    "scheduled_for_utc": "2026-02-03T09:00:00Z"
  }
}
```

Implementation note: the existing `agent.Hook` mechanism can inject this message at `step=0` by modifying the `messages` slice before the first LLM call. The important part is to make `RunOptions.Meta` the single source of truth for injected metadata across all run entrypoints.

### Logging & artifacts
This feature does not require per-run artifact directories or persisted logs. If needed later, add a separate optional “artifacts” mode; the default should store only bounded metadata in SQLite (e.g. `error` and `result_summary`).

## Configuration

### Additions to config (proposal)
Add a scheduler section to `config.example.yaml` and `config.yaml`:
```yaml
scheduler:
  enabled: true
  concurrency: 2
```

Notes:
- Scheduler persistence uses the existing `db.dsn` configuration (SQLite).
- Schedules run in **UTC** (hardcoded).
- `misfire_policy` is hardcoded to `skip`.
- Default timeout is hardcoded to **10 minutes** (per-job `timeout_seconds` can override when provided).

Jobs are created/updated/deleted via internal API/tool calls. Optionally support declarative jobs in config later.

## Security Considerations
- Do not store API keys in the scheduler DB; rely on env vars or existing config practices.
- Do not persist full agent transcripts or tool I/O by default; keep run records small and bounded.

## Failure Modes & Recovery
- Process crash during a run:
  - On restart, runs left in `running` state should be marked as `failed` (or `canceled`) with a note like “process restarted”.
- Persistence unavailable/corrupted:
  - Scheduler should fail fast with a clear error and non-zero exit.
- Long downtime:
  - Apply hardcoded `skip` misfire behavior to avoid run storms.

## Testing Plan (Go)
Add focused tests around:
- Schedule parsing (cron and interval) and next-run computation (UTC).
- Run state transitions (queued/running/terminal).
- Overlap policies (forbid/queue).
- Restart recovery behavior (handling `running` runs).

Integration smoke test (optional):
- Start the scheduler in a test harness with a short interval job.
- Verify at least one run reaches `succeeded` in `cron_runs`.

## Implementation Phases (Suggested)
1) DB schema + migrations (SQLite).
2) Core scheduler loop + worker pool + state transitions.
3) Agent-facing API / internal tools for job lifecycle.
4) Artifacts/logging and restart recovery logic.
5) Update docs and `config.example.yaml`; add tests.

## TODO
- [x] Add SQLite tables + migrations for `cron_jobs` / `cron_runs` (including `status="skipped"`)
- [x] Implement scheduler loop in UTC (cron/interval) with hardcoded misfire behavior `skip`
- [x] Implement overlap `forbid`: emit `slog` + insert `cron_runs.status="skipped"`
- [x] Implement worker pool: claim `queued` runs, run agent, update `cron_runs` terminal status
- [x] Implement restart recovery: mark orphaned `running` runs as failed on boot
- [x] Inject cron metadata via `RunOptions.Meta` (see `docs/feat/feat_20260203_meta.md`)
- [x] Ensure system prompt rule exists so the model treats `mister_morph_meta` as run context (not instructions)
